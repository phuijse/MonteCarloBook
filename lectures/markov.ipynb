{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "hv.opts.defaults(hv.opts.Curve(width=500), \n",
    "                 hv.opts.Histogram(width=500),\n",
    "                 hv.opts.HLine(alpha=0.5, color='r', line_dash='dashed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cadenas de Markov\n",
    "\n",
    "## Introducción\n",
    "\n",
    "En la lección anterior vimos caminatas aleatorias y definimos lo que es un proceso estocástico.  En lo que sigue nos restringiremos a procesos estocásticos que sólo puede tomar valores de un conjunto discreto $\\mathcal{S}$ en tiempos $n>0$ que también son discretos.\n",
    "\n",
    "Llamaremos a $\\mathcal{S}=\\{1, 2, \\ldots, M\\}$ el conjunto de **estados** del proceso. Cada estado en particular se suele denotar por un número natural.\n",
    "\n",
    "Recordemos que para que un proceso estocástico sea considerado una **cadena de Markov**  se debe cumplir \n",
    "\n",
    "$$\n",
    "P(X_{n+1}|X_{n}, X_{n-1}, \\ldots, X_{1}) = P(X_{n+1}|X_{n})\n",
    "$$\n",
    "\n",
    "que se conoce como la propiedad de Markov o propiedad markoviana.\n",
    "\n",
    ":::{important}\n",
    "\n",
    "En una cadena de markov el estado futuro es independiente del pasado cuando conozco el presente\n",
    "\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matriz de transición\n",
    "\n",
    "Si la cadena de Markov tiene estados discretos y es homogenea, podemos escribir\n",
    "\n",
    "$$\n",
    "P(X_{n+1}=j|X_{n}=i) = P_{ij},\n",
    "$$\n",
    "\n",
    "donde homogeneo quiere decir que la probabilidad de transicionar de un estado a otro no cambia con el tiempo. La probabilidad $P_{i,j}$ se suele llamar probabilidad de transición \"a un paso\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El conjunto con todas las posibles combinaciones $P_{ij}$ para $i,j \\in \\mathcal{S}$ forma una matriz cuadrada de $M \\times M$ que se conoce como matriz de transición\n",
    "\n",
    "$$\n",
    "P = \\begin{pmatrix} P_{11} & P_{12} & \\ldots & P_{1M} \\\\ \n",
    "P_{21} & P_{22} & \\ldots & P_{2M} \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "P_{M1} & P_{M2} & \\ldots & P_{MM}\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "donde siempre se debe cumplir que las filas sumen 1\n",
    "\n",
    "$$\n",
    "\\sum_{j \\in \\mathcal{S}} P_{ij} = 1\n",
    "$$\n",
    "\n",
    "y además todos los $P_{ij} \\geq 0$  y $P_{ij} \\in [0, 1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una matriz de transición o matriz estocástica puede representarse como un grafo dirigido donde los vertices son los estados y las aristas las probabilidades de transición o pesos.\n",
    "\n",
    "El siguiente es un ejemplo de grafo para un sistema de cuatro estados con todas sus transiciones equivalentes e iguales a $1/2$. Las transiciones con probabilidad $0$ no se muestran.\n",
    "\n",
    "<img src=\"images/markov2.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considere ahora el siguiente ejemplo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/markov-ruin.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "Si salimos del estado $0$ o del estado $3$ ya no podemos volver a ellos. \n",
    "\n",
    ":::\n",
    "\n",
    "Los estados a los cuales no podemos retornar se conocen como estados **transitorios** o transientes. Por el contrario los estados a los que si tenemos posibilidad de retornar se llaman estados **recurrentes**.\n",
    "\n",
    "En general cuando se tienen estados a los que no se puede retornar se dice que cadena es **reducible**. Por el contrario si podemos regresar a todos los estados se dice que la cadena es **irreducible**.\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Una cadena reducible puede \"dividirse\" para crear cadenas irreducibles. \n",
    "\n",
    ":::\n",
    "\n",
    "En el ejemplo de arriba podemos separar $\\{0\\}$, $\\{1,2\\}$ y $\\{3\\}$ en tres cadenas irreducibles [^ruina]\n",
    "\n",
    "[^ruina]: La cadena de Markov anterior modela un problema conocido como la [ruina del apostador](https://en.wikipedia.org/wiki/Gambler%27s_ruin), puedes estudiar de que se trata [aquí](http://manjeetdahiya.com/posts/markov-chains-gamblers-ruin/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo: Cadena de dos estados\n",
    "\n",
    "Digamos que queremos predecir el clima de Valdivia por medio utilizando una cadena de Markov. Por lo tanto asumiremos que el clima de mañana es perfectamente predecible a partir del clima de hoy. Sean dos estados\n",
    "\n",
    "- $s_A$ Luvioso\n",
    "- $s_B$ Soleado\n",
    "\n",
    "Con probabilidades condicionales $P(s_A|s_A) = 0.7$, $P(s_B|s_A) = 0.3$, $P(s_A|s_B) = 0.45$ y $P(s_B|s_B) = 0.55$. En este caso la matriz de transición es\n",
    "\n",
    "$$ \n",
    "P = \\begin{pmatrix} P(s_A|s_A) & P(s_B|s_A) \\\\ P(s_A|s_B) & P(s_B|s_B) \\end{pmatrix}  = \\begin{pmatrix} 0.7 & 0.3 \\\\ 0.45 & 0.55 \\end{pmatrix} \n",
    "$$\n",
    "\n",
    "que también se puede visualizar como un mapa de transición\n",
    "\n",
    "<img src=\"images/markov1.png\" width=\"500\">\n",
    "\n",
    "Si está soleado hoy, ¿Cuál es la probabilidad de que llueva mañana, en tres dias más y en una semana más? \n",
    "\n",
    "Utilicemos `Python` y la matriz de transición para responder esta pregunta. Primero escribimos la matriz de transición como un `ndarray` de Numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[0.70, 0.30],\n",
    "              [0.45, 0.55]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En segunda lugar vamos a crear un vector de estado inicial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "s0 = np.array([0, 1]) # Estado soleado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego, las probabilidades para mañana dado que hoy esta soleado pueden calcularse como\n",
    "\n",
    "$$\n",
    "s_1 = s_0 P\n",
    "$$\n",
    "\n",
    "que se conoce como transición a un paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(s0, P)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La probabilidad para tres días más puede calcularse como\n",
    "\n",
    "$$\n",
    "s_3 = s_2 P = s_1 P^2 = s_0 P^3\n",
    "$$\n",
    "\n",
    "que se conoce como transición a 3 pasos. Sólo necesitamos elevar la matriz al cubo y multiplicar por el estado inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(s0, np.linalg.matrix_power(P, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El pronóstico para una semana sería entonces la transición a 7 pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(s0, np.linalg.matrix_power(P, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notamos que el estado de nuestro sistema comienza a converger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(s0, np.linalg.matrix_power(P, 1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se conoce como el estado estacionario de la cadena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estado estacionario de la cadena de Markov\n",
    "\n",
    "Si la cadena de Markov converge a un estado, ese estado se llama **estado estacionario**. Una cadena puede tener más de un estado estacionario.\n",
    "\n",
    "Por definición en un estado estacionario se cumple que \n",
    "\n",
    "$$\n",
    "s P = s\n",
    "$$\n",
    "\n",
    "Que corresponde al problema de valores y vectores propios.\n",
    "\n",
    ":::{note}\n",
    "\n",
    "Los estados estacionarios son los vectores propios del sistema\n",
    "\n",
    ":::\n",
    "\n",
    "Para el ejemplo anterior teniamos que\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} s_1 & s_2 \\end{pmatrix} P = \\begin{pmatrix} s_1 & s_2 \\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Que resulta en las siguientes ecuaciones\n",
    "\n",
    "$$\n",
    "0.7 s_1 + 0.45 s_2 = s_1 \n",
    "$$\n",
    "\n",
    "$$\n",
    "0.3 s_1 + 0.55 s_2 = s_2\n",
    "$$\n",
    "\n",
    "Ambas nos dicen que $s_2 = \\frac{2}{3} s_1$. Si además consideramos que $s_1 + s_2 = 1$ podemos despejar y obtener\n",
    "\n",
    "- $s_1 = 3/5 = 0.6$\n",
    "- $s_2 = 0.4$\n",
    "\n",
    "Que es lo que vimos antes. Esto nos dice que en un 60\\% de los días lloverá y en el restante 40% estará soleado"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilidad de transición luego de n-pasos\n",
    "\n",
    "Una pregunta interesante a responder con una cadena de Markov es\n",
    "\n",
    "> ¿Cuál es la probabilidad de llegar al estado $j$ dado que estoy en el estado $i$ si doy exactamente $n$ pasos?\n",
    "\n",
    "Consideremos por ejemplo \n",
    "\n",
    "<img src=\"images/markov3.png\" width=\"400\">\n",
    "\n",
    "donde la matriz de transición es claramente\n",
    "\n",
    "$$\n",
    "P = \\begin{pmatrix} 1/2 & 1/4 & 1/4 \\\\ \n",
    "1/8 & 3/4 & 1/8 \\\\\n",
    "1/4 & 1/4 & 1/2\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Para este ejemplo particular\n",
    "\n",
    "> ¿Cúal es la probabilidad de llegar al estado $2$ desde el estado $0$ en 2 pasos?\n",
    "\n",
    "Podemos resolver esto matemáticamente como\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix} P_{00}  & P_{01} & P_{02} \\end{pmatrix} \\begin{pmatrix} P_{02} \\\\ P_{12} \\\\ P_{22} \\end{pmatrix} = P_{00}P_{02} + P_{01}P_{12} + P_{02}P_{22} = 0.28125 \n",
    "$$\n",
    "\n",
    "Que corresponde al elemento en la fila $0$ y columna $2$ de la matriz $P^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[1/2, 1/4, 1/4],\n",
    "              [1/8, 3/4, 1/8],\n",
    "              [1/4, 1/4, 1/2]])\n",
    "\n",
    "np.dot(P, P)[0, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{important}\n",
    "\n",
    "En general la probabilidad de llegar al estado $j$ desde el estado $i$ en $n$ pasos es equivalente al elemento en la fila $i$ y columna $j$ de la matriz $P^n$\n",
    "\n",
    ":::\n",
    "\n",
    "¿Qué ocurre cuando $n$ tiene a infinito?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(np.linalg.matrix_power(P, 3),\n",
    "        np.linalg.matrix_power(P, 5),\n",
    "        np.linalg.matrix_power(P, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todas las filas convergen a un mismo valor. Este conjunto de probabilidades se conoce como $\\pi$ la distribución estacionaria de la cadena de Markov. Notar que las filas de $P^\\infty$ convergen solo si la cadena es irreducible.\n",
    "\n",
    "El elemento $\\pi_j$ (es decir $P_{ij}^\\infty$) nos da la probabilidad de estar en $j$ luego de infinitos pasos. Notar que el subíndice $i$ ya no tiene importancia, es decir que el punto de partida ya no tiene relevancia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algoritmo general para simular una cadena de Markov discreta\n",
    "\n",
    "Asumiendo que tenemos un sistema con un conjunto discreto de estados $\\mathcal{S}$ y que conocemos la matriz de probabilidades de transición $P$ podemos simular su evolución con el siguiente algoritmo\n",
    "\n",
    "1. Setear $n=0$ y seleccionar un estado inicial $X_n = i$\n",
    "1. Para $n = 1,2,\\ldots,T$\n",
    "    1. Obtener la fila de $P$ que corresponde al estado actual $X_n$, es decir $P[X_n, :]$\n",
    "    1. Generar $X_{n+1}$ muestreando de una distribución multinomial con vector de probabilidad igual a la fila seleccionada \n",
    "\n",
    "En este caso $T$ es el horizonte de la simulación. A continuación veremos como simular una cadena de Markov discreta usando Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Digamos que tenemos una cadena con tres estados y que la fila de $P$ asociada a $X_n$ es $[0.7, 0.2, 0.1]$. Podemos usar `scipy.stats.multinomial` para generar una aleatoriamente una variable multinomial y luego aplicar el argumento máximo para obtener el índice del estado $X_{n+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(scipy.stats.multinomial.rvs(n=1, p=[0.7, 0.2, 0.1], size=1), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si repetimos esto 100 veces se obtiene la siguiente distribución para $X_{n+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.argmax(scipy.stats.multinomial.rvs(n=1, p=[0.7, 0.2, 0.1], size=100), axis=1)\n",
    "edges, bins = np.histogram(x, range=(np.amin(x)-0.5, np.amax(x)+0.5), bins=len(np.unique(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hv.Histogram((edges, bins), kdims='x', vdims='Frecuencia').opts(xticks=[0, 1, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo cual coincide con la fila de $P$ que utilizamos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que sabemos como obtener el estado siguiente probemos algo un poco más complicado.\n",
    "\n",
    "Consideremos el ejemplo de predicción de clima y simulemos 1000 cadenas a un horizonte de 10 pasos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([[0.70, 0.30],\n",
    "              [0.45, 0.55]])\n",
    "\n",
    "n_chains = 1000\n",
    "horizon = 10\n",
    "states = np.zeros(shape=(n_chains, horizon), dtype='int')\n",
    "states[:, 0] = 1 # Estado inicial para todas las cadenas\n",
    "\n",
    "for i in range(n_chains):\n",
    "    for j in range(1, horizon):\n",
    "        states[i, j] = np.argmax(scipy.stats.multinomial.rvs(n=1, p=P[states[i, j-1], :], size=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestran las tres primeras simulaciones como series de tiempo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p =[]\n",
    "for i in range(3):\n",
    "    p.append(hv.Curve((states[i, :]), 'n', 'Estados').opts(yticks=[0, 1]))\n",
    "hv.Overlay(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestra el estado más probable en cada paso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_states = len(np.unique(states))\n",
    "\n",
    "hist = np.zeros(shape=(horizon, n_states))\n",
    "for j in range(horizon):\n",
    "    hist[j, :] = np.array([sum(states[:, j] == s) for s in range(n_states)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hv.Curve((np.argmax(hist, axis=1)), 'n', 'Estado más probable').opts(yticks=[0, 1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ley de los grandes números para variables no i.i.d.\n",
    "\n",
    "Previamente vimos que el promedio de un conjunto de $N$ variables independientes e idénticamente distribuidas (iid) converge a su valor esperado cuando $N$ es grande.\n",
    "\n",
    "Por ejemplo \n",
    "\n",
    "$$\n",
    "\\lim_{N \\to \\infty} \\frac{1}{N} \\sum_{i=1}^N X_i = \\mu\n",
    "$$\n",
    "\n",
    "En esta lección vimos que la cadena de markov, un proceso estocástico donde no se cumple el supuesto iid, puede tener en ciertos casos una distribución estacionaria \n",
    "\n",
    ":::{note}\n",
    "\n",
    "La **distribución estacionaria** $\\pi$ de una cadena de Markov con matriz de transición $P$ es tal que $\\pi P = \\pi$\n",
    "\n",
    ":::\n",
    "\n",
    "**Teorema de ergodicidad:** Una cadena de Markov irreducible y aperiodica tiene una distribución estacionaria $\\pi$ única, independiente de valor del estado inicial y que cumple\n",
    "\n",
    "$$\n",
    "\\lim_{n\\to \\infty} s_j(n) = \\pi_j\n",
    "$$\n",
    "\n",
    "donde los componentes de $\\pi$ representan la fracción de tiempo que la cadena estará en cada uno de los estados luego de observarla por un largo tiempo\n",
    "\n",
    ":::{important}\n",
    "\n",
    "El límite de observar la cadena por un tiempo largo es análogo al análisis de estadísticos estáticos sobre muestras grandes. Esto es el equivalente a la ley de los grandes números para el caso de la cadena de Markov\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    "### Notas históricas\n",
    "\n",
    "- **La primera ley de los grandes números:** [Jacob Bernoulli](https://en.wikipedia.org/wiki/Jacob_Bernoulli) mostró la primera versión de la Ley de los grandes números en su Ars Conjectandi en 1713. Esta primera versión parte del supuesto de que las VAs son iid. Bernoulli era un firme creyente del destino, se oponía al libre albedrío y abogaba por el determinismo en los fenómenos aleatorios.\n",
    "- **La segunda ley de los grandes números:** En 1913 el matemático ruso [Andrei Markov](https://en.wikipedia.org/wiki/Andrey_Markov) celebró el bicentenario de la famosa prueba de Bernoulli organizando un simposio donde presentó su nueva versión de la Ley de los grandes números que aplica sobre la clase de procesos estocásticos que hoy llamamos procesos de Markov, de esta forma extendiendo el resultado de Bernoulli a un caso que no es iid.\n",
    "- **La pugna de Markov y Nekrasov:** En aquellos tiempos Markov estaba en pugna con otro matemático ruso: [Pavel Nekrasov](https://en.wikipedia.org/wiki/Pavel_Nekrasov). Nekrasov había publicado previamente que \"la independencia era una condición necesaria para que se cumpla la ley de los grandes números\". Nekrasov mantenia que el comportamiento humano al no ser iid no podía estar guiado por la ley de los grandes números, es decir que los humanos actuan voluntariamente y con libre albedrío. Markov reaccionó a esta afirmación desarrollando un contra-ejemplo que terminó siendo lo que hoy conocemos como los procesos de Markov "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
