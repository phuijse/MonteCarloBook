{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones generales \n",
    "\n",
    "1. Forme un grupo de **máximo tres estudiantes**\n",
    "1. Versione su trabajo usando un **repositorio <font color=\"red\">privado</font> de github**. Agregue a sus compañeros y a su profesor (usuario github: phuijse) en la pestaña *Settings/Manage access*. No se aceptarán consultas si la tarea no está en github. No se evaluarán tareas que no estén en github.\n",
    "1. Se evaluará el **resultado, la profundidad de su análisis y la calidad/orden de sus códigos** en base al último commit antes de la fecha y hora de entrega\". Se bonificará a quienes muestren un método de trabajo incremental y ordenado según el histórico de *commits*\n",
    "1. Sean honestos, ríganse por el [código de ética de la ACM](https://www.acm.org/about-acm/code-of-ethics-in-spanish)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mi primera Red Neuronal Bayesiana\n",
    "\n",
    "Las redes neuronales son modelos del estado del arte para hacer regresión y clasificación con datos complejos\n",
    "\n",
    "Generalmente estos modelos requieren de una gran cantidad de datos para poder entrenarlos de forma efectiva y sin que se sobreajusten. Sin embargo, en algunos problemas los datos disponibles son simplemente muy escasos o muy difíciles de obtener. Adicionalmente, no es directo tomar decisiones en base al modelo, y se requiere un paso adicional de calibración. ¿Cómo podemos confiar en las decisiones del modelo?\n",
    "\n",
    "Podemos intentar solucionar estos problemas escribiendo la red neuronal como un modelo bayesiano y aprender el posterior de sus parámetros con un método de Markov Chain Monte Carlo (siempre y cuando el modelo sea simple). \n",
    "\n",
    "Incorporando priors el modelo estará regularizado y en lugar de estimadores puntuales tendremos la distribución a posteriori completa. Esta rica información extra nos permite medir la confianza del modelo sobre sus predicciones (el modelo sabe cuando no sabe) facilitando la tarea de calibración. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formulación clásica\n",
    "\n",
    "En esta tarea se pide que programen un modelo de red neuronal para clasificación de datos bidimensionales, de dos clases, con una capa oculta y con función de activación sigmoidal\n",
    "\n",
    "Sea el conjunto de datos y etiquetas\n",
    "\n",
    "$$\n",
    "\\mathcal{D} = \\{(x, y)^{(i)}, i=1,2,\\ldots,N\\} \\quad x^{(i)} \\in \\mathbb{R}^2,  y^{(i)} \\in \\{0, 1\\}\n",
    "$$\n",
    "\n",
    "Consideremos ahora una tupla en particular $(X, Y)$. La salida de la capa oculta en notación matricial es\n",
    "\n",
    "$$\n",
    "Z = \\text{sigmoide}( W_Z X + B_Z)\n",
    "$$\n",
    "\n",
    "donde $W_Z \\in \\mathbb{R}^{M \\times 2}$, $B_Z \\in \\mathbb{R}^{M}$ y  $M$ es el tamaño de la capa oculta\n",
    "\n",
    "La salida de la capa visible (última capa) en notación matricial es\n",
    "\n",
    "$$\n",
    "Y = \\text{sigmoide}( W_Y Z + B_Y)\n",
    "$$\n",
    "\n",
    "donde $W_Y \\in \\mathbb{R}^{1 \\times M}$, $B_Z \\in \\mathbb{R}$\n",
    "\n",
    "La función sigmoide se define como\n",
    "\n",
    "$$\n",
    "\\text{sigmoide}(x) = \\frac{1}{1+ e^{-x}}\n",
    "$$\n",
    "\n",
    "Luego $Z$ es un vector de largo $M$ con valores en $[0, 1]$ e $Y$ es un escalar con valor en el rango $[0, 1]$\n",
    "\n",
    "## Formulación bayesiana\n",
    "\n",
    "Para darle un toque bayesiano a este modelo debemos\n",
    "\n",
    "- Definir priors para $W_Z$, $B_Z$, $W_Y$ y $B_Y$. Se pide que utilice priors **normales con media cero y desviación estándar diez**.\n",
    "- Definir una verosimilitud para le problema. Dado que el problema es de clasificación binaria, utilice una distribución de **Bernoulli** con $p=Y$\n",
    "- Considere los datos $X$ como una variable determínista. \n",
    "\n",
    "## Indicaciones\n",
    "\n",
    "Utilice\n",
    "\n",
    "- El atributo `shape` para darle la dimensión correcta a cada variable cada uno\n",
    "- El atributo `observed` para asignar las etiquetas reales a esta variable aleatoria observada\n",
    "- `pm.Data` para la variable independiente\n",
    "- `theano.tensor.sigmoid` para calcular la función sigmoide\n",
    "- `A.dot(B)` para calcular el producto matricial entre `A` y `B`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instrucciones específicas\n",
    "\n",
    "- Considere el dataset sintético `two-moons` que se muestra a continuación. Se pide que realice dos experimentos variando el valor de `n_samples`, primero a $100$ y finalmente a $10$\n",
    "- Implemente el modelo de red neuronal bayesiana en `pymc3` dejando $M$ como un argumento. Para cada valor de `n_samples` entrene tres modelos con $M=1$, $M=3$ y $M=10$\n",
    "- Seleccione y calibre un algoritmo de MCMC para entrenar este modelo. Justifique y respalde sus decisiones en base al comportamiento de las trazas, al estadístico Gelman-Rubin y a la función de autocorrelación\n",
    "- Estudie el posterior de los parámetros y evalue el posterior predictivo sobre los datos de prueba. Muestre graficamente la media y la varianza del posterior predictivo en el espacio de los datos. Haga observaciones y comparaciones entre los 6 casos (3 valores de $M$ y 2 valores de `n_samples`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "x, y = make_moons(n_samples=100, # Varie este parámetro\n",
    "                  shuffle=True, noise=0.2, random_state=123456)\n",
    "x = (x - np.mean(x, keepdims=True))/np.std(x, keepdims=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "ax.scatter(x[y==0, 0], x[y==0, 1], marker='o')\n",
    "ax.scatter(x[y==1, 0], x[y==1, 1], marker='x')\n",
    "\n",
    "x1, x2 = np.meshgrid(np.linspace(-3, 3, 100), \n",
    "                     np.linspace(-3, 3, 100))\n",
    "x_test = np.vstack([x1.ravel(), x2.ravel()]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: Si `model_preds` es el posterior predictivo en el conjunto de test donde la primera dimensión son las muestras y la segunda dimensión los ejemplos, podemos graficar la media de ese posterior como:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "cmap = ax.pcolormesh(x1, x2, np.mean(model_preds, axis=0).reshape(len(x1), len(x2)), \n",
    "                     cmap=plt.cm.RdBu_r, shading='gouraud', vmin=0, vmax=1)\n",
    "plt.colorbar(cmap, ax=ax)\n",
    "ax.scatter(x[y==0, 0], x[y==0, 1], c='k', marker='o')\n",
    "ax.scatter(x[y==1, 0], x[y==1, 1], c='k', marker='x')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INFO183",
   "language": "python",
   "name": "info183"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
