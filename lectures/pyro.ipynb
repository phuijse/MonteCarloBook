{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Programación probabilística (PP)\n",
    "\n",
    "Nuevo paradigma que busca combinar los lenguajes de programación de propósito general con el modelamiento probabilístico\n",
    "\n",
    "El objetivo es hacer estadística y en particular inferencia Bayesiana usando las herramientas de ciencias de la computación\n",
    "\n",
    "Como veremos a continuación la PP corre en dos direcciones:\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1809.10756\"><img src=\"images/PP.png\" width=\"500\"></a>\n",
    "\n",
    "El lenguaje Python tiene un ecosistema rico en frameworks y librerías de PP:\n",
    "\n",
    "- [PyMC3](https://docs.pymc.io/notebooks/getting_started.html)\n",
    "- [PyStan](https://pystan.readthedocs.io/en/latest/)\n",
    "- [Edward](http://edwardlib.org/)\n",
    "- [Pyro](http://pyro.ai)\n",
    "- [emcee](http://dfm.io/emcee/current/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial de PyMC3\n",
    "\n",
    "En este tutorial veremos como\n",
    "\n",
    "1. Definir un modelo bayesiano en base a una verosimilitud y a un prior\n",
    "1. Aplicar distintos algoritmos de MCMC sobre el modelo\n",
    "1. Verificar la convergencia y analizar los posteriors\n",
    "\n",
    "Usaremos como ejemplo una **regresión lineal Bayesiana**\n",
    "\n",
    "## Instalación\n",
    "\n",
    "Usando los canales por defecto de conda instalar usando\n",
    "\n",
    "    conda install pymc3 theano graphviz python-graphviz\n",
    "    \n",
    "Graphviz solo se requiere para algunos gráficos opcionales\n",
    "\n",
    "Adicionalmente \n",
    "\n",
    "    conda install corner -c conda-forge\n",
    "    \n",
    "También para hacer algunos gráficos de diagnóstico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "## Formulación clásica\n",
    "\n",
    "Consideramos que tenemos $N$ tuplas $(x_i, y_i)$ donde $X$ es la variable independiente e $Y$ la dependiente\n",
    "\n",
    "En una regresión queremos estimar $\\mathbb{E}[Y|X]$ en base a un modelo paramétrico $Y = f_\\theta(X)$\n",
    "\n",
    "En este caso asumiremos un modelo lineal\n",
    "\n",
    "$$\n",
    "y_i = w x_i + b + \\epsilon \\quad i=1,\\ldots,N\n",
    "$$\n",
    "\n",
    "donde queremos aprender los parámetros $w$ y $b$ bajo el supuesto de que $p(\\epsilon) = \\mathcal{N}(0, \\sigma^2)$\n",
    "\n",
    "Luego \n",
    "\n",
    "$$\n",
    "y \\sim  \\mathcal{N}(b + w x, \\sigma^2)\n",
    "$$\n",
    "\n",
    "y la verosimilitud sería\n",
    "\n",
    "$$\n",
    "p(y|x,w,b,\\sigma)  = \\prod_{i=1}^N \\mathcal{N}(b + w x_i, \\sigma^2)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports y creación de datos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc3 as pm\n",
    "print(f'PyMC3 ver: {pm.__version__}')\n",
    "import theano.tensor as T\n",
    "\n",
    "np.random.seed(123456)\n",
    "b_star, w_star, sigma_star, N = 1, 2.5, 1., 10\n",
    "X = np.random.randn(N)\n",
    "Y = b_star + w_star*X +  np.random.randn(N)*sigma_star\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "ax.scatter(X, Y, c='k')\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y'); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Formulación bayesiana\n",
    "\n",
    "A diferencia de una regresión \"convencional\" asumiremos que $w$, $b$ y $\\sigma$ no son variables deterministas sino **aleatorias** y por ende **tienen distribuciones**\n",
    "\n",
    "<img src=\"images/lin_reg_plate.png\" width=\"700\">\n",
    "\n",
    "\n",
    "Lo que buscamos es el posterior de los parámetros, en este caso usando el Teorema de Bayes\n",
    "\n",
    "$$\n",
    "p(w, b, \\sigma|D) = \\frac{p(D|w, b, \\sigma) p(w) p(b) p(\\sigma)}{\\int p(D|w, b, \\sigma) p(w) p(b) p(\\sigma) dw db d\\sigma}\n",
    "$$\n",
    "\n",
    "donde $D$ representa los datos $(x, y)$ y por simplicidad el prior se puede factorizar\n",
    "\n",
    "Para obtener muestras de este posterior usaremos MCMC\n",
    "\n",
    "\n",
    "## Especificación del modelo generativo en PyMC3\n",
    "\n",
    "El modelo generativo es aquel que \"produjo\" los datos\n",
    "\n",
    "Usualmente parte en los hiperparámetros, continua en las variables latentes (priors) y termina en las variables observadas (verosimilitud)\n",
    "\n",
    "En PyMC3 los modelos se deben crear dentro de un contexto `pm.Model()`\n",
    "\n",
    "Dentro de este contexto definimos los priors, la verosimilitud y cualquier otra variable auxiliar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as bayes_reg:\n",
    "    # Datos de entrada\n",
    "    X_shared = pm.Data(\"x\", X)\n",
    "    # Priors\n",
    "    b = pm.Normal('b', mu=0, sd=10, shape=())\n",
    "    w = pm.Normal('w', mu=0, sd=10, shape=())\n",
    "    sigma = pm.Lognormal('sigma', sigma=1, testval=np.std(Y))\n",
    "    # Variable determinista\n",
    "    mu = pm.Deterministic('mu', b + X_shared*w)  \n",
    "    # Verosimilitud\n",
    "    Y_obs = pm.Normal('Y_obs', mu, sigma, observed=Y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Creando variables aleatorias\n",
    "\n",
    "Para crear una variable aleatoria debemos primero seleccionar una distribución por ejemplo `Normal`, `Bernoulli`, etc\n",
    "\n",
    "El constructor debe especificar el nombre propio de la variable (primer argument) y los parámetros de la distribución\n",
    "\n",
    "Opcionalmente se puede proporcional el valor inicial de la variable aleatoria con `testval` y la dimensionalidad de la variable aleatoria con `shape`\n",
    "\n",
    "\n",
    "\n",
    "La lista completa de distribuciones la pueden revisar [aquí](https://docs.pymc.io/api/distributions.html)\n",
    "\n",
    "### Especificando la verosimilitud\n",
    "\n",
    "La verosimilitud se crea igual que una variable aleatoria pero agregando el argumento `observed`\n",
    "\n",
    "A este argumento debemos proporcionarle los datos en formato `ndarray` de NumPy o `DataFrame` de pandas\n",
    "\n",
    "### Jerarquía\n",
    "\n",
    "Note como usamos variables aleatorias como parámetros para otras variables aleatorias\n",
    "\n",
    "El modelo toma nota de esto y crea relaciones padre-hijo entre los nodos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "display(bayes_reg.free_RVs, \n",
    "        bayes_reg.deterministics, \n",
    "        bayes_reg.observed_RVs)\n",
    "\n",
    "display(bayes_reg)\n",
    "\n",
    "pm.model_to_graphviz(bayes_reg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Qué genera nuestro modelo?\n",
    "\n",
    "Un primer diagnóstico para asegurarnos de que nuestro modelo hace lo que queremos que haga es observar las muetras que están generando\n",
    "\n",
    "Podemos muestrear desde los priors usando la función `pm.sample_prior_predictive`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-5, 5, num=100)\n",
    "\n",
    "with bayes_reg:\n",
    "    prior_checks = pm.sample_prior_predictive(samples=50, var_names=['w', 'b'])\n",
    "    \n",
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y'); \n",
    "\n",
    "for w, b in zip(prior_checks['w'], prior_checks['b']):\n",
    "    ax.plot(x_test, x_test*w + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## MCMC con PyMC3\n",
    "\n",
    "La maquinaria de MCMC en PyMC3 se accede usando la función \n",
    "\n",
    "```python\n",
    "pm.sample(draws=500, # Largo de la traza (posterior a burn-in)\n",
    "          step=None, # Algoritmo de propuestas\n",
    "          chains=None, # Número de cadenas\n",
    "          cores=None, # Número de núcleos de CPU\n",
    "          tune=500, # Número de pasos de burn-in\n",
    "          init='auto', # Algoritmo de inicialización \n",
    "          n_init=200000, # Número de iteraciones del algoritmo de inicialización\n",
    "          start=None, # Diccionario con valores iniciales para las RV \n",
    "          ...\n",
    "          )\n",
    "```\n",
    "\n",
    "El largo de la traza y la cantidad de iteraciones de *burn-in* debemos afinarlas iterativamente inspeccionando las trazas y las divergencias de la cadena. Más es siempre mejor pero también más lento\n",
    "\n",
    "Es recomendable usar dos o más cadenas en paralelo, así podemos verificar con mayor certeza la convergencia\n",
    "\n",
    "El argumento `step` define el algoritmo de propuestas. Podemos usar un mismo algoritmo para todos los parámetros o usar uno distinto para cada uno. Entre los algoritmos se [encuentran](https://docs.pymc.io/api/inference.html#step-methods) \n",
    "\n",
    "- `pm.Metropolis()`: Metropolis Hastings, el cual ya hemos visto en detalle. Es la opción por defecto para variables discretas\n",
    "- `pm.NUTS()`: [No U-turn Sampler](https://arxiv.org/abs/1111.4246), un *sampler* de tipo Hamiltoniano. Es la opción por defecto para variables continuas\n",
    "\n",
    "El punto de partida de la cadena es muy relevante. Está la opción de entregar valores manualmente con el argumento `start` o usar un método automático y más económico que MCMC, por defecto se inicializa con una heurística que depende de cada *sampler*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Breve intermedio teórico) Hamiltonian Monte Carlo\n",
    "\n",
    "Es una familia de métodos de propuesta que usan información de la derivada de la densidad de la cual estamos muestreando para hacer transiciones más eficientes. Por ende sólo funcionan con parámetros continuos y derivables\n",
    "\n",
    "Cada iteración es más costosa con respecto a Metropolis-Hastings, pero en general se requieren menos iteraciones ya que converge más rápido al estado estacionario\n",
    "\n",
    "Revisemos los siguientes [ejemplos animados](http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html). [Más detalles](https://mc-stan.org/docs/2_21/reference-manual/hamiltonian-monte-carlo.html)  del algoritmo [HMC original](https://arxiv.org/abs/1312.0906)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obteniendo muestras con `pm.sample`\n",
    "\n",
    "De vuelta a nuestro problema, veamos como cambian los resultados con distintos *samplers* y largos de traza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "with pm.Model() as bayes_reg:\n",
    "    # Datos de entrada\n",
    "    X_shared = pm.Data(\"x\", X)\n",
    "    # Priors\n",
    "    b = pm.Normal(name='b', mu=0, sd=10, shape=())\n",
    "    w = pm.Normal(name='w', mu=0, sd=10, shape=())\n",
    "    sigma = pm.Lognormal('sigma', sigma=1, testval=np.std(Y))\n",
    "    # Variable determinista\n",
    "    mu = pm.Deterministic('mu', b + X_shared*w)  \n",
    "    # Verosimilitud\n",
    "    Y_obs = pm.Normal('Y_obs', mu, sigma, observed=Y)\n",
    "\n",
    "    \n",
    "with bayes_reg:\n",
    "    trace = pm.sample(draws=1000, tune=500, chains=2, cores=2,\n",
    "                      step=pm.NUTS())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnósticos visuales y numéricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos visualizar los posteriors y las trazas con `traceplot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.traceplot(trace, figsize=(6, 3), var_names=['b', 'w', 'sigma'], combined=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si sólo nos interesan los posteriors y sus descriptores podemos usar `plot_posterior`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pm.plot_posterior(trace, figsize=(8, 2), var_names=['b', 'w', 'sigma'], textsize=12);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O un resumen tabular de lo anterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.summary(trace, var_names=['b', 'w', 'sigma']).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde $\\hat r$ es el estadístico Gelman Rubin \n",
    "\n",
    "- Se compara la varianza entre múltiples cadenas con la varianza interna de cada cadena\n",
    "- Valores mayores que uno indican que una o más cadenas aun no han convergido\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente podemos estudiar convergencia usando la función de autocorrelación para cada parámetro y cada cadena como vimos en la lección anterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.plots.autocorrplot(trace, figsize=(6, 4), \n",
    "                      var_names=['w', 'b', 'sigma']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos visualizar las correlaciones en el posterior "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corner import corner\n",
    "samples = np.vstack([trace[k] for k in [\"w\", \"b\", \"sigma\"]]).T\n",
    "corner(samples, quantiles=[0.16, 0.5, 0.84],\n",
    "       labels=[r\"$w$\", r\"$b$\", r\"$\\sigma$\"],\n",
    "       show_titles=True, title_kwargs={\"fontsize\": 12});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distribución posterior predictiva\n",
    "\n",
    "Ahora que tenemos el posterior de los parámetros podemos usarlo para calcular la distribución posterior predictiva en función de un nuevo dato $\\textbf{x}$\n",
    "\n",
    "$$\n",
    "p(\\textbf{y}|\\textbf{x}, \\mathcal{D}) = \\int p(\\textbf{y}|\\textbf{x},\\theta) p(\\theta| \\mathcal{D}) \\,d\\theta \n",
    "$$\n",
    "\n",
    "donde en este caso $\\theta = (w, b, \\sigma)$ y se asume que $y$ es condicionalmente independiente de  $\\mathcal{D}$ dado que conozco $\\theta$\n",
    "\n",
    "La parte más difícil era estimar $p(\\theta| \\mathcal{D})$ el cual ya tenemos. Para obtener las muestras del posterior predictivo podemos usar nuestra traza con la función `pm.sample_posterior_predictive`\n",
    "\n",
    "Note como se actualiza la variable compartida `x` para que refleje los datos de prueba que queremos predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-5, 5, num=100)\n",
    "\n",
    "with bayes_reg:\n",
    "    pm.set_data({\"x\": x_test})\n",
    "    posterior_predictive = pm.sample_posterior_predictive(trace, samples=50, \n",
    "                                                          var_names=[\"mu\", \"Y_obs\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y'); \n",
    "\n",
    "for line in posterior_predictive[\"mu\"]:\n",
    "    ax.plot(x_test, line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 3), tight_layout=True)\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y'); \n",
    "\n",
    "mean = np.mean(posterior_predictive[\"mu\"], axis=0)\n",
    "perc = np.percentile(posterior_predictive[\"mu\"], q=(5, 95), axis=0)\n",
    "\n",
    "ax.plot(x_test, mean)\n",
    "ax.fill_between(x_test, perc[1], perc[0], alpha=0.5)\n",
    "ax.scatter(X, Y, c='k')\n",
    "ax.set_xlabel('X'); ax.set_ylabel('Y');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INFO183",
   "language": "python",
   "name": "info183"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
