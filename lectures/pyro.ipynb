{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "hv.opts.defaults(hv.opts.Curve(width=500), \n",
    "                 hv.opts.Scatter(width=500, size=10), \n",
    "                 hv.opts.HLine(alpha=0.5, color='k', line_dash='dashed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "# import corner\n",
    "\n",
    "\"\"\"\n",
    "Para instalar pyro sugiero usar conda y pip\n",
    "\n",
    "    conda install -c pytorch pytorch pip\n",
    "    pip install pyro-ppl\n",
    "\n",
    "\"\"\"\n",
    "import torch\n",
    "import pyro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Programación probabilística \n",
    "\n",
    "## Introducción\n",
    "\n",
    "La programación probabilística (PP) es un nuevo paradigma que busca combinar los lenguajes de programación de propósito general con el modelamiento probabilístico.\n",
    "\n",
    "El objetivo es hacer estadística y en particular inferencia Bayesiana usando las herramientas de ciencias de la computación. Como muestra el siguiente diagrama la PP corre en dos direcciones:\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1809.10756\"><img src=\"images/PP.png\" width=\"500\"></a>\n",
    "\n",
    "El lenguaje Python tiene un ecosistema rico en frameworks y librerías de PP:\n",
    "\n",
    "- [PyMC3](https://docs.pymc.io/notebooks/getting_started.html)\n",
    "- [PyStan](https://pystan.readthedocs.io/en/latest/)\n",
    "- [Edward](http://edwardlib.org/)\n",
    "- [Pyro](http://pyro.ai)\n",
    "- [emcee](http://dfm.io/emcee/current/)\n",
    "\n",
    "\n",
    "\n",
    "En este tutorial aprenderemos a usar  la librería `pyro` \n",
    "\n",
    "1. Definir un modelo bayesiano en base a una verosimilitud y a un prior\n",
    "1. Aplicar distintos algoritmos de MCMC sobre el modelo\n",
    "1. Verificar la convergencia y analizar los posteriors\n",
    "\n",
    "Usaremos como ejemplo una **regresión lineal Bayesiana**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regresión lineal bayesiana\n",
    "\n",
    "Consideramos que tenemos $N$ tuplas $(x_i, y_i)$ donde $X$ es la variable independiente e $Y$ la dependiente\n",
    "\n",
    "En una regresión queremos estimar $\\mathbb{E}[Y|X]$ en base a un modelo paramétrico $Y = f_\\theta(X)$. En este caso asumiremos un modelo lineal\n",
    "\n",
    "$$\n",
    "y_i = w x_i + b + \\epsilon \\quad i=1,2,\\ldots,N\n",
    "$$\n",
    "\n",
    "donde queremos aprender los parámetros $\\theta=(w, b)$ bajo el supuesto de que $p(\\epsilon) = \\mathcal{N}(0, \\sigma^2)$\n",
    "\n",
    "Luego \n",
    "\n",
    "$$\n",
    "y \\sim  \\mathcal{N}(b + w x, \\sigma^2)\n",
    "$$\n",
    "\n",
    "y por lo tanto la verosimilitud sería\n",
    "\n",
    "$$\n",
    "p(y|x,w,b,\\sigma)  = \\prod_{i=1}^N \\mathcal{N}(b + w x_i, \\sigma^2)\n",
    "$$\n",
    "\n",
    "A diferencia de una regresión \"convencional\" asumiremos que $w$, $b$ y $\\sigma$ no son variables determinísticas sino **aleatorias** y por ende **tienen distribuciones**. Llamamos *prior* a la distribución de los parámetros previo a observar nuestros datos. \n",
    "\n",
    "En este caso particular asumiremos una distribución normal para los priors de $w$ y $b$\n",
    "\n",
    "$$\n",
    "p(b) = \\mathcal{N}(\\mu_b, \\sigma_b^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(w) = \\mathcal{N}(\\mu_w, \\sigma_w^2)\n",
    "$$\n",
    "\n",
    "Lo que buscamos es el posterior de los parámetros $\\theta$, es decir su distribución condicionado a nuestras observaciones $D$\n",
    "\n",
    "$$\n",
    "p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{p(D)}\n",
    "$$\n",
    "\n",
    "En este caso particular el posterior es\n",
    "\n",
    "$$\n",
    "p(w, b, \\sigma|D) = \\frac{p(D|w, b, \\sigma) p(w) p(b) p(\\sigma)}{\\int p(D|w, b, \\sigma) p(w) p(b) p(\\sigma) dw db d\\sigma}\n",
    "$$\n",
    "\n",
    "donde por simplicidad se asume que $p(w,b,\\sigma) = p(w)p(b)p(\\sigma)$, es decir que el prior no tiene correlaciones entre los parámetros.\n",
    "\n",
    ":::{important}\n",
    "\n",
    "Estimaremos este posterior en base a muestras utilizando MCMC\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La librería `pyro` utiliza `pytorch` como backend.\n",
    "\n",
    "Esto significa que debemos definir o transformar nuestros datos a formato `torch.Tensor` para poder entregarlos a nuestros modelos. \n",
    "\n",
    "A continuación se generan los datos que utilizaremos a lo largo de este tutorial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyro.set_rng_seed(1234)\n",
    "\n",
    "b_star, w_star, sigma_star, N = 1, 2.5, 1., 10\n",
    "x = torch.randn(N)\n",
    "y = b_star + w_star*x +  torch.randn(N)*sigma_star\n",
    "\n",
    "x_test = torch.linspace(-3, 3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hv.Scatter((x, y)).opts(color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Especificación del modelo generativo \n",
    "\n",
    "El modelo generativo es aquel que \"produjo\" los datos. Usualmente comienza con los hiperparámetros, continua en las variables latentes (priors) y termina en las variables observadas (verosimilitud). A continuación se muestra el diagrama de placas de la regresión lineal bayesiana con todas sus variables, parámetros e hiperparámetros\n",
    "\n",
    "<img src=\"images/lin_reg_plate.png\" width=\"600\">\n",
    "\n",
    "\n",
    "En `pyro` un modelo generativo es simplemente una función que define y relaciona las variables determinísticas y aleatorias que utilizaremos. Para crear una variable aleatoria se utiliza la primitiva\n",
    "\n",
    "```python\n",
    "pyro.sample(name, # El nombre de la variable (string)\n",
    "            fn, # La distribución de la variable\n",
    "            obs, # (Opcional) Los datos observados asociados a esta variable\n",
    "            ...)\n",
    "```\n",
    "\n",
    "Los priors son variables aleatorias que no usan el argumento `obs`. En cambio, para escribir la verosimilitud, utilizamos el argumento `obs` para proporcionar los datos. Las distribuciones conocidas se pueden importar desde `pyro.distributions`. \n",
    "\n",
    "Para definir una variable determínistica se utiliza la primitiva\n",
    "\n",
    "```python\n",
    "pyro.deterministic(name, # El nombre de la variable (string)\n",
    "                   value, # Transformación sobre otras variables del modelo\n",
    "                   ...\n",
    "                   )\n",
    "```\n",
    "\n",
    "A continuación se muestra la implementación del modelo de regresión lineal en `pyro`. Utilizaremos $\\mu_w = \\mu_b = 0$ y $\\sigma_w = \\sigma_b = 10.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HalfNormal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.distributions import Normal, HalfNormal\n",
    "\n",
    "def model(x, y=None):\n",
    "    w = pyro.sample(\"w\", Normal(0.0, 10.0))\n",
    "    b = pyro.sample(\"b\", Normal(0.0, 10.0))\n",
    "    s = pyro.sample(\"s\", HalfNormal(1.0))\n",
    "    with pyro.plate('datos', size=len(x)):\n",
    "        f = pyro.deterministic('f', x*w + b)\n",
    "        pyro.sample(\"y\", Normal(f, s), obs=y)    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante la definición del modelo también utilizamos la primitiva\n",
    "\n",
    "```python\n",
    "pyro.plate(name, # El nombre del contexto (string)\n",
    "           size=None, # El tamaño del dataset (int)\n",
    "           device=None, # Se puede escoger entre cpu o gpu\n",
    "           ...\n",
    "           )\n",
    "```\n",
    "para crear una variable `y` que está condicionada al conjunto de variables `x`. Internamente `pyro.plate` también se hace cargo de paralelizar operaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un primer diagnóstico para asegurarnos de que nuestro modelo está definido correctamente es observar las muestras que están generando.\n",
    "\n",
    "Podemos muestrear desde nuestro modelo utilizando\n",
    "\n",
    "```python\n",
    "pyro.infer.Predictive(model, # El modelo que definimos anteriormente\n",
    "                      num_samples=None, # El número de muestras que deseamos generar\n",
    "                      return_sites=(), # Las variables de las cuales deseamos muestrear\n",
    "                      posterior_samples=None, # Opcional: Lo veremos más adelante\n",
    "                      ...\n",
    "                     )\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, return_sites=(\"w\", \"b\", \"s\", \"f\"), num_samples=2000)\n",
    "prior_samples = predictive(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La variable `samples` contiene un diccionario cuyas llaves son las variables seleccionadas en `return_sites` y valores son tensores de 1000 elementos. \n",
    "\n",
    "En base a las muestras podemos construir histogramas o gráficos de densidad como se muestra a continuación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "joint = hv.Bivariate((prior_samples['w'], prior_samples['b']), \n",
    "                     kdims=['w', 'b']).opts(cmap='Blues', line_width=0, filled=True)\n",
    "\n",
    "wmarginal, bmarginal = ((hv.Distribution(joint, kdims=[dim])) for dim in 'wb')\n",
    "(joint) << bmarginal.opts(width=125) << wmarginal.opts(height=125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribuciones estimadas a partir de las muestras son consistentes con los priors que escogimos.\n",
    "\n",
    "Adicionalmente podemos estudiar el espacio de posibles modelos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p = []\n",
    "for curve in prior_samples['f'][:100, :]:\n",
    "    p.append(hv.Curve((x_test, curve)).opts(color='#30a2da', alpha=0.25))\n",
    "hv.Overlay(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se conoce como **distribución prior predictiva**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC con pyro\n",
    "\n",
    "La maquinaria de MCMC en `pyro` se accede usando la función \n",
    "\n",
    "```python\n",
    "pyro.infer.MCMC(kernel, # Un algoritmo muestreador, por ejemplo Metropolis\n",
    "                num_samples, # Largo de la traza (sin contar burn-in)\n",
    "                warmup_steps=None, # Cantidad de muestras iniciales a descartar\n",
    "                initial_params=None, # (Opcional) Valores iniciales para la cadena\n",
    "                num_chains=1, # Número de cadena (se pueden correr en paralelo)\n",
    "                ... \n",
    "               )\n",
    "``` \n",
    "\n",
    "Los principales métodos de `infer.MCMC` son\n",
    "\n",
    "- `run()`: Realiza el muestreo y pobla las cadenas, espera los mismos argumentos que la función `model`\n",
    "- `diagnostics()`: Retorna diagnósticos de la convergencia de la cadena (como Gelman-Rubin)\n",
    "- `summary()`: Retorna una tabla con los momentos estadísticos de los parámetros\n",
    "- `get_sample()`: Retorna la traza, es decir las muestras del posterior\n",
    "\n",
    "El argumento más importante de `infer.MCMC` es el `kernel`. Actualmente hay dos disponibles: `HMC` ([Hamiltonian Monte Carlo](https://arxiv.org/abs/1312.0906)) y `NUTS` ([No-U turn sampler](https://arxiv.org/abs/1111.4246)). Ambos son muestreadores para parámetros continuos que utilizan información del gradiente para proponer transiciones.\n",
    "\n",
    "En general, cada iteración de HMC/NUTS es más costosa con respecto a Metropolis-Hastings, pero en general se requieren menos iteraciones ya que converge más rápido al estado estacionario. Recomiendo revisar los siguientes [ejemplos animados](http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html) para tener una idea conceptual de la diferencia entre Metropolis y HMC.\n",
    "\n",
    "NUTS es ampliamente considerado como el estado del arte en algoritmos de propuestas para paramétros continuos. Veamos a continuación como se muestrea usando MCMC y NUTS con `pyro`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import MCMC, NUTS\n",
    "\n",
    "sampler = MCMC(kernel=NUTS(model, adapt_step_size=True), \n",
    "               num_chains=2, num_samples=1000, warmup_steps=100)\n",
    "\n",
    "sampler.run(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de usar el posterior es muy recomendable diagnosticar la adecuada convergencia de las cadenas. En primer lugar  podemos utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.diagnostics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De donde podemos resaltar que\n",
    "\n",
    "- El número de muestras efectivo es alto (del orden de 50%)\n",
    "- El estadísitco de Gelman Rubin es cercano a 1 para ambos parámetros\n",
    "- No hubieron divergencias durante el muestro\n",
    "\n",
    "Todo indicativos de una buena convergencia. También podemos obtener las trazas utilizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = sampler.get_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se visualizan las trazas de `w`, `b` y `s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p1 = hv.Curve((posterior_samples['w']), 'Iteraciones', 'Traza', label='w')\n",
    "p2 = hv.Curve((posterior_samples['b']), 'Iteraciones', 'Traza', label='b')\n",
    "p3 = hv.Curve((posterior_samples['s']), 'Iteraciones', 'Traza', label='s')\n",
    "\n",
    "hv.Overlay([p1, p2, p3]).opts(legend_position='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como vimos la lección anterior la autocorrelación de la traza es una excelente herramienta para diagnosticar la correcta operación del algoritmo. En este caso la autocorrelación es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(theta_trace):\n",
    "    thetas_norm = (theta_trace-np.mean(theta_trace))/np.std(theta_trace)\n",
    "    rho = np.correlate(thetas_norm, \n",
    "                       thetas_norm, mode='full')\n",
    "    return rho[len(rho) // 2:]/len(theta_trace)\n",
    "\n",
    "rho = {}\n",
    "for param in ['w', 'b', 's']:\n",
    "    rho[param] = autocorrelation(posterior_samples[param].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p = []\n",
    "for key, value in rho.items():\n",
    "    p.append(hv.Curve((value), 'Retardo', 'Traza', label=key))\n",
    "\n",
    "hv.Overlay(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "Para ambos parametros la autocorrelación decrece rapidamente y se mantiene en torno a cero\n",
    "\n",
    ":::\n",
    "\n",
    "Las métricas y diagnósticos nos indican que el algoritmo MCMC convergió al estado estacionario. Por lo tanto podemos  inspeccionar y utilizar el posterior para nuestro modelo de regresión lineal sin preocupaciones.\n",
    "\n",
    "En primer lugar podemos obtener algunos estadísticos de los posterior con"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.summary(prob=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "También podemos explorar los posterior de `w` y `b` con estimadores de densidad basados en las muestras de la traza como se muestra a continuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "joint = hv.Bivariate((posterior_samples['w'], posterior_samples['b']), \n",
    "                     kdims=['w', 'b']).opts(cmap='Blues', line_width=0, filled=True)\n",
    "\n",
    "wmarginal, bmarginal = ((hv.Distribution(joint, kdims=[dim])) for dim in 'wb')\n",
    "(joint) << bmarginal.opts(width=125) << wmarginal.opts(height=125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "Claramente el posterior $p(\\theta| \\mathcal{D})$ se ha desplazado con respecto al prior $p(\\theta)$ que vimos anteriormente\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos el posterior de los parámetros podemos usarlo para calcular la **distribución posterior predictiva** en función de nuevos datos $\\textbf{x}$\n",
    "\n",
    "$$\n",
    "p(\\textbf{y}|\\textbf{x}, \\mathcal{D}) = \\int p(\\textbf{y}|\\textbf{x},\\theta) p(\\theta| \\mathcal{D}) \\,d\\theta \n",
    "$$\n",
    "\n",
    "donde en este caso $\\theta = (w, b, \\sigma)$ y se asume que $y$ es condicionalmente independiente de  $\\mathcal{D}$ dado que conozco $\\theta$.\n",
    "\n",
    "La parte más difícil era estimar $p(\\theta| \\mathcal{D})$ el cual ya tenemos gracias a MCMC. Para obtener muestras del posterior predictivo podemos nuevamente usar la clase `predictive` pero ahora le entregramos las muestras del posterior como argumento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = pyro.infer.Predictive(model, return_sites=([\"f\"]), \n",
    "                                   posterior_samples=posterior_samples)\n",
    "samples = predictive(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente figura aparecen los datos como puntos negros y 100 muestras del posterior predictivo (lineas azules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = hv.Scatter((x, y), label='datos').opts(color='k')\n",
    "p = []\n",
    "for curve in samples['f'][:100, :]:\n",
    "    p.append(hv.Curve((x_test, curve)).opts(color='#30a2da', alpha=0.1))\n",
    "hv.Overlay(p) * data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{important}\n",
    "\n",
    "Nuestro modelo bayesiano nos retorna una distribución de soluciones\n",
    "\n",
    ":::\n",
    "\n",
    "También podemos presentar la distribución graficamente usando estadísticos como se muestra a continuación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p5, p50, p95 = samples['f'].quantile(torch.tensor([0.05, 0.5, 0.95]), axis=0)\n",
    "line = hv.Curve((x_test, p50), label='mediana')\n",
    "shade = hv.Spread((x_test, p50, p95-p5), label='95% CI').opts(color='#30a2da', alpha=0.5)\n",
    "hv.Overlay([line, shade, data]).opts(legend_position='bottom_right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{important}\n",
    "\n",
    "Con el posterior podemos estudiar no sólo la solución más probable sino también el rango de las soluciones. El rango o ancho de la distribución está relacionado a la incertidumbre de nuestro modelo y observaciones (ruido)\n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
