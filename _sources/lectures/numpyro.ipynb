{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "hv.opts.defaults(hv.opts.Curve(width=500), \n",
    "                 hv.opts.Scatter(width=500, size=10), \n",
    "                 hv.opts.HLine(alpha=0.5, color='k', line_dash='dashed'), \n",
    "                 hv.opts.VLine(alpha=0.5, color='k', line_dash='dashed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Para instalar numpyro sugiero usar conda\n",
    "\n",
    "    conda install -c conda-forge numpyro\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import numpyro\n",
    "from jax import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Programación probabilística \n",
    "\n",
    "## Introducción\n",
    "\n",
    "La programación probabilística (PP) es un nuevo paradigma que busca combinar los lenguajes de programación de propósito general con el modelamiento probabilístico.\n",
    "\n",
    "El objetivo es hacer estadística y en particular inferencia Bayesiana usando las herramientas de ciencias de la computación. Como muestra el siguiente diagrama la PP corre en dos direcciones:\n",
    "\n",
    "<a href=\"https://arxiv.org/abs/1809.10756\"><img src=\"images/PP.png\" width=\"500\"></a>\n",
    "\n",
    "El lenguaje Python tiene un ecosistema rico en frameworks y librerías de PP:\n",
    "\n",
    "- [PyMC3](https://docs.pymc.io/notebooks/getting_started.html)\n",
    "- [PyStan](https://pystan.readthedocs.io/en/latest/)\n",
    "- [emcee](http://dfm.io/emcee/current/)\n",
    "- [Edward](http://edwardlib.org/)\n",
    "- [Pyro](http://pyro.ai) y [NumPyro](http://num.pyro.ai/en/latest/index.html)\n",
    "\n",
    "\n",
    "En este tutorial aprenderemos a usar la librería `NumPyro` para:\n",
    "\n",
    "1. Definir un modelo bayesiano en base a una verosimilitud y a un prior\n",
    "1. Aplicar distintos algoritmos de MCMC sobre el modelo\n",
    "1. Verificar la convergencia y analizar los posteriors\n",
    "\n",
    "A grandes rasgos `NumPyro` opera como una especie de interfaz entre la librería NumPy de computo numérico y el backed `pyro` de programación probabilística. Entre sus ventajas se encuentra el uso de [JAX](https://github.com/google/jax) para hacer compilación *just in time* en CPU/GPU, lo cual la hace muy eficiente.\n",
    "\n",
    "\n",
    "Como ejemplo para aprender a usar esta librería utilizaremos un modelo de **regresión lineal Bayesiana**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Regresión lineal bayesiana\n",
    "\n",
    "Consideramos que tenemos $N$ tuplas $(x_i, y_i)$ donde $X$ es la variable independiente e $Y$ la dependiente\n",
    "\n",
    "En una regresión queremos estimar $\\mathbb{E}[Y|X]$ en base a un modelo paramétrico $Y = f_\\theta(X)$. En este caso asumiremos un modelo lineal\n",
    "\n",
    "$$\n",
    "y_i = w x_i + b + \\epsilon \\quad i=1,2,\\ldots,N\n",
    "$$\n",
    "\n",
    "donde queremos aprender los parámetros $\\theta=(w, b)$ bajo el supuesto de que $p(\\epsilon) = \\mathcal{N}(0, \\sigma_\\epsilon^2)$\n",
    "\n",
    "Luego \n",
    "\n",
    "$$\n",
    "y \\sim  \\mathcal{N}(b + w x, \\sigma_\\epsilon^2)\n",
    "$$\n",
    "\n",
    "y por lo tanto la verosimilitud sería\n",
    "\n",
    "$$\n",
    "p(y|x,w,b,\\sigma)  = \\prod_{i=1}^N \\mathcal{N}(b + w x_i, \\sigma_\\epsilon^2)\n",
    "$$\n",
    "\n",
    "A diferencia de una regresión \"convencional\" asumiremos que $w$, $b$ y $\\sigma_\\epsilon$ no son variables determinísticas sino **aleatorias** y por ende **tienen distribuciones**. Llamamos *prior* a la distribución de los parámetros previo a observar nuestros datos. \n",
    "\n",
    "En este caso particular asumiremos una distribución normal para los priors de $w$ y $b$\n",
    "\n",
    "$$\n",
    "p(b) = \\mathcal{N}(\\mu_b, \\sigma_b^2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "p(w) = \\mathcal{N}(\\mu_w, \\sigma_w^2)\n",
    "$$\n",
    "\n",
    "Lo que buscamos es el posterior de los parámetros $\\theta$, es decir su distribución condicionado a nuestras observaciones $D$\n",
    "\n",
    "$$\n",
    "p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{p(D)}\n",
    "$$\n",
    "\n",
    "En este caso particular el posterior es\n",
    "\n",
    "$$\n",
    "p(w, b, \\sigma_\\epsilon|D) = \\frac{p(D|w, b, \\sigma_\\epsilon) p(w) p(b) p(\\sigma_\\epsilon)}{\\int p(D|w, b, \\sigma_\\epsilon) p(w) p(b) p(\\sigma_\\epsilon) \\, dw \\, db \\, d\\sigma_\\epsilon}\n",
    "$$\n",
    "\n",
    "donde por simplicidad se asume que $p(w,b,\\sigma_\\epsilon) = p(w)p(b)p(\\sigma_\\epsilon)$, es decir que el prior no tiene correlaciones entre los parámetros.\n",
    "\n",
    ":::{important}\n",
    "\n",
    "Estimaremos este posterior en base a muestras utilizando MCMC\n",
    "\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se generan los datos que utilizaremos a lo largo de este tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1234)\n",
    "\n",
    "b_star, w_star, s_eps_star, N = 10, 2.5, 1., 10\n",
    "x = np.random.randn(N)\n",
    "y = b_star + w_star*x +  np.random.randn(N)*s_eps_star\n",
    "\n",
    "x_test = np.linspace(-3, 3, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "hv.Scatter((x, y)).opts(color='k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Especificación del modelo generativo \n",
    "\n",
    "El modelo generativo es aquel que \"produjo\" los datos. Usualmente comienza con los hiperparámetros, continua en las variables latentes (priors) y termina en las variables observadas (verosimilitud). A continuación se muestra el diagrama de placas de la regresión lineal bayesiana con todas sus variables, parámetros e hiperparámetros\n",
    "\n",
    "<img src=\"images/lin_reg_plate.png\" width=\"600\">\n",
    "\n",
    "\n",
    "En `NumPyro` un modelo generativo es simplemente una función que define y relaciona las variables determinísticas y aleatorias que utilizaremos. Para crear una variable aleatoria se utiliza la primitiva\n",
    "\n",
    "```python\n",
    "numpyro.sample(name, # El nombre de la variable (string)\n",
    "               fn, # La distribución de la variable\n",
    "               obs, # (Opcional) Los datos observados asociados a esta variable\n",
    "               rng_key, # Una semilla aleatoria\n",
    "               ...\n",
    "              )\n",
    "```\n",
    "\n",
    "Los priors son variables aleatorias que no usan el argumento `obs`. En cambio, para escribir la verosimilitud, utilizamos el argumento `obs` para proporcionar los datos. Las distribuciones conocidas se pueden importar desde `numpyro.distributions`. \n",
    "\n",
    "Para definir una variable determínistica se utiliza la primitiva\n",
    "\n",
    "```python\n",
    "numpyro.deterministic(name, # El nombre de la variable (string)\n",
    "                      value, # Transformación sobre otras variables del modelo\n",
    "                      )\n",
    "```\n",
    "\n",
    "A continuación se muestra la implementación del modelo de regresión lineal en `pyro`. Utilizaremos una distribución normal con $\\mu_w = \\mu_b = 0$ y $\\sigma_w = \\sigma_b = 5.0$ para $w$ y $b$. Para $\\sigma_\\epsilon$ utilizaremos una distribución Cauchy con parámetro de escala $\\gamma = 5.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpyro.distributions as dist\n",
    "\n",
    "def model(x, y=None):\n",
    "    w = numpyro.sample(\"w\", dist.Normal(loc=0.0, scale=5.0))\n",
    "    b = numpyro.sample(\"b\", dist.Normal(loc=0.0, scale=5.0))\n",
    "    s_eps = numpyro.sample(\"s\", dist.HalfCauchy(scale=5.0))\n",
    "    with numpyro.plate('datos', size=len(x)):\n",
    "        f = numpyro.deterministic('f', value=x*w + b)\n",
    "        numpyro.sample(\"y\", dist.Normal(loc=f, scale=s_eps), obs=y)\n",
    "\n",
    "#numpyro.contrib.render.get_model_relations(model, model_args=(x, y))\n",
    "#trace = numpyro.handlers.trace(numpyro.handlers.seed(model, 0)).get_trace(x, y)\n",
    "#print(numpyro.util.format_shapes(trace))\n",
    "#numpyro.render_model(model, (x, y)) # Requiere instalar graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Durante la definición del modelo también utilizamos la primitiva\n",
    "\n",
    "```python\n",
    "numpyro.plate(name, # El nombre del contexto (string)\n",
    "              size=None, # El tamaño del dataset (int)\n",
    "              subsample_size=None, # El tamaño del minibatch (opcional)\n",
    "              ...\n",
    "             )\n",
    "```\n",
    "para crear una variable `y` que está condicionada al conjunto de variables `x`. Internamente `numpyro.plate` también se hace cargo de paralelizar operaciones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de utilizar MCMC sobre nuestro modelo generativo es conveniente observar las muestras que genera. De esta forma podemos detectar tempranamente si cometimos un error en la definición del modelo\n",
    "\n",
    "Podemos lograr lo anterior construyendo la \"distribución predictiva\" de nuestro modelo. Podemos realizar esto utilizando  \n",
    "\n",
    "```python\n",
    "numpyro.infer.Predictive(model, # El modelo que definimos anteriormente\n",
    "                         num_samples=None, # El número de muestras que deseamos generar\n",
    "                         return_sites=(), # Las variables de las cuales deseamos muestrear\n",
    "                         posterior_samples=None, # Opcional: Lo veremos más adelante\n",
    "                         ...\n",
    "                        )\n",
    "```\n",
    "\n",
    "lo cual crea un objeto que podemos utilizar para evaluar nuestro modelo y recuperar las muestras de sus variables aleatorias y determinísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creamos el objetivo Predictive\n",
    "predictive = numpyro.infer.Predictive(model, \n",
    "                                      num_samples=2000)\n",
    "\n",
    "# Para muestrear el objeto predictive requiere una semilla aleatoria y las variables de entrada del modelo\n",
    "rng_key = random.PRNGKey(12345)\n",
    "rng_key, rng_key_ = random.split(rng_key)\n",
    "prior_samples = predictive(rng_key_, x_test)\n",
    "prior_samples.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En base a las muestras podemos construir histogramas o gráficos de densidad como se muestra a continuación. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "joint = hv.Bivariate((prior_samples['w'], prior_samples['b']), \n",
    "                     kdims=['w', 'b']).opts(cmap='Blues', line_width=0, filled=True)\n",
    "\n",
    "wmarginal, bmarginal = ((hv.Distribution(joint, kdims=[dim])) for dim in 'wb')\n",
    "(joint) << bmarginal.opts(width=125) << wmarginal.opts(height=125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La distribuciones estimadas a partir de las muestras son consistentes con los priors que escogimos.\n",
    "\n",
    "Adicionalmente podemos estudiar el espacio de posibles modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "data = hv.Scatter((x, y), label='datos').opts(color='k')\n",
    "p = []\n",
    "for curve in prior_samples['f'][:100, :]:\n",
    "    p.append(hv.Curve((x_test, curve)).opts(color='#30a2da', alpha=0.1))\n",
    "\n",
    "p5, p50, p95 = np.quantile(prior_samples['f'], (0.05, 0.5, 0.95), axis=0)\n",
    "line = hv.Curve((x_test, p50), label='mediana')\n",
    "shade = hv.Spread((x_test, p50, p95-p5), label='95% CI').opts(color='#30a2da', alpha=0.5)\n",
    "\n",
    "hv.Layout([hv.Overlay(p) * data, \n",
    "           hv.Overlay([line, shade, data]).opts(legend_position='bottom_right')]).opts(hv.opts.Curve(width=350))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esto se conoce como **distribución prior predictiva**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MCMC con pyro\n",
    "\n",
    "La maquinaria de MCMC en `numpyro` se accede usando la función \n",
    "\n",
    "```python\n",
    "numpyro.infer.MCMC(sampler, # Un algoritmo muestreador, por ejemplo Metropolis\n",
    "                   num_warmup, # Cantidad de muestras iniciales a descartar\n",
    "                   num_samples, # Largo de la traza (sin contar burn-in)\n",
    "                   num_chains=1, # Número de cadenas\n",
    "                   thinning=1, # Cuantas muestras por medio de la traza se preservan\n",
    "                   ... \n",
    "                  )\n",
    "``` \n",
    "\n",
    "Los principales métodos de [`infer.MCMC`](http://num.pyro.ai/en/stable/mcmc.html) son\n",
    "\n",
    "- `run()`: Realiza los cálculos para poblar las cadenas, espera una semilla aleatorio y los mismos argumentos que la función `model`\n",
    "- `print_summary()`: Retorna una tabla con los momentos estadísticos de los parámetros y algunos diagnósticos\n",
    "- `get_sample()`: Retorna la traza, es decir las muestras del posterior\n",
    "\n",
    "El argumento más importante de `infer.MCMC` es el `sampler`. Entre los algoritmos disponibles se encuentran: `HMC` ([Hamiltonian Monte Carlo](https://arxiv.org/abs/1312.0906)) y `NUTS` ([No-U turn sampler](https://arxiv.org/abs/1111.4246)). Ambos son muestreadores para parámetros continuos que utilizan información del gradiente para proponer transiciones.\n",
    "\n",
    "Cada iteración de HMC/NUTS es más costosa con respecto a Metropolis-Hastings, pero en general se requieren menos iteraciones ya que converge más rápido al estado estacionario. \n",
    "\n",
    "Recomiendo revisar los siguientes [ejemplos animados](http://arogozhnikov.github.io/2016/12/19/markov_chain_monte_carlo.html) para tener una idea conceptual de la diferencia entre Metropolis y HMC.\n",
    "\n",
    "NUTS es ampliamente considerado como el estado del arte en algoritmos de propuestas para paramétros continuos. Veamos a continuación como se muestrea usando MCMC y NUTS con `numpyro`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "rng_key, rng_key_ = random.split(rng_key)\n",
    "\n",
    "sampler = numpyro.infer.MCMC(sampler=numpyro.infer.NUTS(model), \n",
    "                             num_samples=1000, num_warmup=100, thinning=1,\n",
    "                             num_chains=2)\n",
    "\n",
    "sampler.run(rng_key_, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de usar el posterior es muy recomendable diagnosticar la adecuada convergencia de las cadenas. En primer lugar  podemos utilizar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler.print_summary(prob=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De donde podemos resaltar que\n",
    "\n",
    "- El estadísitco de Gelman Rubin es cercano a 1 para todos los parámetros\n",
    "- El número de muestras efectivo es alto \n",
    "- No hubieron divergencias durante el muestro\n",
    "\n",
    "Todo indicativos de una buena convergencia. También podemos obtener las trazas utilizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posterior_samples = sampler.get_samples()\n",
    "posterior_samples.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se visualizan las trazas de `w`, `b` y `s`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p1 = hv.Curve((posterior_samples['w']), 'Iteraciones', 'Traza', label='w')\n",
    "p2 = hv.Curve((posterior_samples['b']), 'Iteraciones', 'Traza', label='b')\n",
    "p3 = hv.Curve((posterior_samples['s']), 'Iteraciones', 'Traza', label='s')\n",
    "\n",
    "hv.Overlay([p1, p2, p3]).opts(legend_position='top')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La autocorrelación de la traza es una excelente herramienta para diagnosticar la correcta operación del algoritmo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorrelation(theta_trace):\n",
    "    thetas_norm = (theta_trace-np.mean(theta_trace))/np.std(theta_trace)\n",
    "    rho = np.correlate(thetas_norm, \n",
    "                       thetas_norm, mode='full')\n",
    "    return rho[len(rho) // 2:]/len(theta_trace)\n",
    "\n",
    "rho = {}\n",
    "for param in ['w', 'b', 's']:\n",
    "    rho[param] = autocorrelation(posterior_samples[param])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso la autocorrelación de las trazas es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "p = []\n",
    "for key, value in rho.items():\n",
    "    p.append(hv.Curve((value), 'Retardo', 'Traza', label=key).opts(alpha=0.5))\n",
    "\n",
    "hv.Overlay(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "Para todos los parametros la autocorrelación decrece rápidamente y se mantiene en torno a cero\n",
    "\n",
    ":::\n",
    "\n",
    "Las métricas y diagnósticos nos indican que el algoritmo MCMC convergió al estado estacionario. Por lo tanto podemos  inspeccionar y utilizar el posterior para nuestro modelo de regresión lineal sin preocupaciones.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se muestran los posterior de `w` y `b` en base a estimadores de densidad construidos con las muestras de la traza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "joint = hv.Bivariate((posterior_samples['w'], posterior_samples['b']), \n",
    "                     kdims=['w', 'b']).opts(cmap='Blues', line_width=0, filled=True)\n",
    "\n",
    "wmarginal, bmarginal = ((hv.Distribution(joint, kdims=[dim])) for dim in 'wb')\n",
    "(joint) << bmarginal.opts(width=125) * hv.VLine(b_star) << wmarginal.opts(height=125) * hv.VLine(w_star)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{note}\n",
    "\n",
    "Claramente el posterior $p(\\theta| \\mathcal{D})$ se ha desplazado con respecto al prior $p(\\theta)$ que vimos anteriormente. Además está muy cercano a los valores \"reales\" que generaron los datos (linea punteada negra)\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos el posterior de los parámetros podemos usarlo para calcular la **distribución posterior predictiva** en función de nuevos datos $\\textbf{x}$\n",
    "\n",
    "$$\n",
    "p(\\textbf{y}|\\textbf{x}, \\mathcal{D}) = \\int p(\\textbf{y}|\\textbf{x},\\theta) p(\\theta| \\mathcal{D}) \\,d\\theta \n",
    "$$\n",
    "\n",
    "donde en este caso $\\theta = (w, b, \\sigma)$ y se asume que $y$ es condicionalmente independiente de  $\\mathcal{D}$ dado que conozco $\\theta$.\n",
    "\n",
    "La parte más difícil era estimar $p(\\theta| \\mathcal{D})$ el cual ya tenemos gracias a MCMC. Para obtener muestras del posterior predictivo podemos nuevamente usar la clase `predictive` pero ahora le entregramos las muestras del posterior como argumento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive = numpyro.infer.Predictive(model, \n",
    "                                      return_sites=([\"f\"]), \n",
    "                                      posterior_samples=posterior_samples)\n",
    "posterior_predictive_samples = predictive(random.PRNGKey(1), x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En la siguiente figura aparecen los datos como puntos negros y 100 muestras del posterior predictivo (lineas azules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "data = hv.Scatter((x, y), label='datos').opts(color='k')\n",
    "p = []\n",
    "for curve in posterior_predictive_samples['f'][:100, :]:\n",
    "    p.append(hv.Curve((x_test, curve)).opts(color='#30a2da', alpha=0.1))\n",
    "\n",
    "p5, p50, p95 = np.quantile(posterior_predictive_samples['f'], (0.05, 0.5, 0.95), axis=0)\n",
    "line = hv.Curve((x_test, p50), label='mediana')\n",
    "shade = hv.Spread((x_test, p50, p95-p5), label='95% CI').opts(color='#30a2da', alpha=0.5)\n",
    "\n",
    "hv.Layout([hv.Overlay(p) * data, \n",
    "           hv.Overlay([line, shade, data]).opts(legend_position='bottom_right')]).opts(hv.opts.Curve(width=350))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{important}\n",
    "\n",
    "Nuestro modelo bayesiano nos retorna una distribución de soluciones\n",
    "\n",
    ":::\n",
    "\n",
    "\n",
    ":::{important}\n",
    "\n",
    "Con el posterior podemos estudiar no sólo la solución más probable sino también el rango de las soluciones. El rango o ancho de la distribución está relacionado a la incertidumbre de nuestro modelo y observaciones (ruido)\n",
    "\n",
    ":::"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
